{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#May require to install opencv if it is not available: conda install -c menpo opencv\n",
    "\n",
    "#REMEMBER: Open CV used BGR image encoding format\n",
    "import cv2\n",
    "\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import pickle #to save and load python objects\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the constants\n",
    "SIZE = 10 #Grid size of environment will be 10*10\n",
    "EPISODES = 25000\n",
    "MOVE_PENALTY = 1\n",
    "ENEMY_PENALTY = 300\n",
    "FOOD_REWARD = 25\n",
    "epsilon = 1 #epsilon greedy policy\n",
    "EPS_DECAY = 0.9998\n",
    "SHOW_EVERY = 3000\n",
    "start_q_table = None #can be prior q table saved using pickle library\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_N=1\n",
    "FOOD_N=2\n",
    "ENEMY_N=3\n",
    "#open cv using BGR format for image representation\n",
    "d={1:(255,0,0),\n",
    "   2:(0,255,0),\n",
    "   3:(0,0,255)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating class for player, food and enemy\n",
    "class Blob:\n",
    "    def __init__(self): #Python constructor\n",
    "        self.x = np.random.randint(0,SIZE)\n",
    "        self.y = np.random.randint(0,SIZE)\n",
    "        #To Do: There can be a issue in which enemy and/or player and/or food land on the same cell\n",
    "    \n",
    "    #for debugging purposes\n",
    "    def __str__(self):\n",
    "        return f\"{self.x}, {self.y}\"\n",
    "    \n",
    "    #operator overloading\n",
    "    def __sub__(self,second):\n",
    "        return (self.x - second.x, self.y-second.y)\n",
    "        \n",
    "    #defining possible actions by Blob agent\n",
    "    def action(self,choice): #very simplified discrete action space consisting of four actions only\n",
    "        if choice == 0:\n",
    "            self.move(x=1,y=1)\n",
    "        elif choice==1:\n",
    "            self.move(x=-1,y=-1)\n",
    "        elif choice ==2:\n",
    "            self.move(x=-1,y=1)\n",
    "        elif choice ==3:\n",
    "            self.move(x=1,y=-1)\n",
    "        elif choice == 4:#Move right\n",
    "            self.move(x=1,y=0)\n",
    "        elif choice ==5 :#Move left\n",
    "            self.move(x=-1,y=0)\n",
    "        elif choice ==6: #Move Up\n",
    "            self.move(x=0,y=1)\n",
    "        elif choice ==7: #Move Down\n",
    "            self.move(x=0,y=-1)\n",
    "        #To Do: Add more choices\n",
    "    \n",
    "    def move(self, x=False,y=False):\n",
    "        #The agent will move either randomly or based on value passed in x or y\n",
    "        #if not x:#x is local var and self.x is class var\n",
    "        #    self.x = np.random.randint(-1,2)  \n",
    "        #else:\n",
    "        self.x += x\n",
    "            \n",
    "        #if not y:\n",
    "        #    self.y = np.random.randint(-1,2)  \n",
    "        #else:\n",
    "        self.y += y\n",
    "        \n",
    "        #We have to also ensure that blob does not move outside the boundaries\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > (SIZE-1):\n",
    "            self.x = SIZE-1\n",
    "        \n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > (SIZE-1):\n",
    "            self.y = SIZE-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#States of our Q Table consist of difference between x and y coordinates of the player and food Blob AND player and enemy Blob\n",
    "if start_q_table is None:\n",
    "    q_table = {} #dictionary\n",
    "    for x1 in range(-SIZE+1, SIZE):\n",
    "        for y1 in range(-SIZE+1, SIZE):\n",
    "            for x2 in range(-SIZE+1, SIZE):\n",
    "                for y2 in range(-SIZE+1,SIZE):\n",
    "                    q_table[((x1,y1),(x2,y2))]= [np.random.uniform(-5,0) for i in range(8)] #since there are eight discrete actions\n",
    "                    #The initial values need to be modified to see the impact\n",
    "                    \n",
    "else: #The Q Table exists and may be present in the form of pickle object\n",
    "    with open(start_q_table,\"rb\") as f:\n",
    "        q_table = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple function to display the state of environment\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "def display(player,food,enemy):\n",
    "    os.system('clear')\n",
    "    env_list = [[None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None],\n",
    "                [None, None, None, None, None, None, None, None, None, None]]\n",
    "\n",
    "    for i in range(0,SIZE):\n",
    "        for j in range(0,SIZE):\n",
    "            '''print(f'i:{i} j:{j}')\n",
    "            print(f'Player {i==player.x and j==player.y}')\n",
    "            print(f'Food {i==food.x and j==food.y}')\n",
    "            print(f'Enemy {i==enemy.x and j==enemy.y}')'''\n",
    "            env_list[i][j]= ' '\n",
    "            if ((i==player.x) and (j==player.y)):\n",
    "                env_list[i][j]='P'\n",
    "            if ((i==food.x) and (j==food.y)):\n",
    "                env_list[i][j]='F'\n",
    "            if ((i==enemy.x) and (j==enemy.y)):\n",
    "                env_list[i][j]='E'\n",
    "    return env_list\n",
    "'''p=Blob()\n",
    "f=Blob()\n",
    "e=Blob()\n",
    "print(f'Player {p.x} {p.y}')\n",
    "print(f'Food {f.x} {f.y}')\n",
    "print(f'Enemy {e.x} {e.y}')'''\n",
    "'''temp=display(p,f,e)\n",
    "for i in range(SIZE):\n",
    "    print(temp[i])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_EPSILON_DECAYING = 1#From which episode we want to start to decay epsilon\n",
    "END_EPSILON_DECAYING = EPISODES // 2 #Till which episode we want to decay epsilon\n",
    "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "episode_rewards = []\n",
    "for episode in range(EPISODES):\n",
    "    player = Blob()\n",
    "    food = Blob()\n",
    "    enemy = Blob()\n",
    "    \n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f'on #{episode}, epsilon: {epsilon}')\n",
    "        print(f'{SHOW_EVERY} ep mean {np.mean(episode_rewards[-SHOW_EVERY: ])}')\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "    \n",
    "    #updating epsilon value\n",
    "    \n",
    "    episode_reward = 0\n",
    "    for i in range(200): #Here 200 is the steps in each episode. It is a hyperparameter\n",
    "        obs = (player - food, player - enemy)#Remember the function overriding\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[obs])\n",
    "        else:\n",
    "            action = np.random.randint(0,8)\n",
    "        \n",
    "        player.action(action)\n",
    "       \n",
    "        if episode>12000:\n",
    "            env_state = display(player, food, enemy)\n",
    "            #time.sleep(0.5)#delay in sec\n",
    "            '''print(f'Player {player.x} {player.y}')\n",
    "            print(f'Food {food.x} {food.y}')\n",
    "            print(f'Enemy {enemy.x} {enemy.y}')'''\n",
    "            print(f'Episode: {episode} Step: {i}',end='')\n",
    "            print(f'Mean Reward{np.mean(episode_rewards[-SHOW_EVERY: ])}')\n",
    "            for i in range(SIZE):\n",
    "                print('|',end='')\n",
    "                for j in range(SIZE):\n",
    "                    print(f' {env_state[i][j]}|',end='')\n",
    "                print('') \n",
    "        #Deciding the reward or penalty of agent after every step in each episode\n",
    "        if player.x == enemy.x and player.y == enemy.y:\n",
    "            reward = -ENEMY_PENALTY\n",
    "        elif player.x == food.x and player.y == food.y:\n",
    "            reward = FOOD_REWARD\n",
    "        else:\n",
    "            reward = -MOVE_PENALTY\n",
    "            \n",
    "        new_obs = (player-food,player-enemy)\n",
    "        max_future_q = np.max(q_table[new_obs])\n",
    "        current_q = q_table[obs][action]\n",
    "    \n",
    "        if reward == FOOD_REWARD:\n",
    "            new_q = FOOD_REWARD\n",
    "            q_table[obs][action] = new_q\n",
    "            episode_rewards.append(reward)\n",
    "            break\n",
    "        elif reward == -ENEMY_PENALTY:\n",
    "            new_q = -ENEMY_PENALTY\n",
    "            q_table[obs][action] = new_q\n",
    "            episode_rewards.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[obs][action] = new_q\n",
    "            episode_rewards.append(reward)\n",
    "    \n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=0\n",
    "not x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
