{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Understanding the problem : Frozen Lake\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "![](frozen.png)\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "- SFFF       (S: starting point, safe)\n",
    "- FHFH       (F: frozen surface, safe)\n",
    "- FFFH       (H: hole, fall to your doom)\n",
    "- HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Setting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym.spaces.tuple_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c874e6ca966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m \u001b[0;31m#to clear output of previous execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;31m#for delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple_space\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m \u001b[0;31m#??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;31m# for using custom environment of Frozen Lake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;31m#for random number generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym.spaces.tuple_space'"
     ]
    }
   ],
   "source": [
    "# src1 (not working)- https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/\n",
    "# src2 https://harderchoices.com/2018/04/04/monte-carlo-method-in-python/\n",
    "import gym #For model of RL Problem\n",
    "import numpy as np #for numpy array anf matrices\n",
    "import operator #??\n",
    "from IPython.display import clear_output #to clear output of previous execution\n",
    "from time import sleep #for delay\n",
    "from gym.spaces.tuple_space import Tuple #??\n",
    "from gym.envs.registration import register# for using custom environment of Frozen Lake\n",
    "import random #for random number generation\n",
    "import itertools #??\n",
    "#import tqdm\n",
    "#tqdm.monitor_interval = 0\n",
    "\n",
    "#Multiple execution of this cell will result re registration of environment which will generate error\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=200)\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200)\n",
    "    \n",
    "fl_slippery = {\n",
    "    'small': 'FrozenLake-v0',\n",
    "    'big': 'FrozenLake8x8-v0'\n",
    "}\n",
    "\n",
    "fl_not_slippery = {\n",
    "    'small': 'FrozenLakeNotSlippery-v0',\n",
    "    'big': 'FrozenLakeNotSlippery8x8-v0'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(slippery=False, big=False):\n",
    "    if slippery:\n",
    "        env = gym.make(fl_slippery['big'] if big else fl_slippery['small'])\n",
    "    else:\n",
    "        env = gym.make(fl_not_slippery['big'] if big else fl_not_slippery['small'])\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):#for each state\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):#for each action corresponding to each state\n",
    "            p[action] = 1 / env.action_space.n#equal probabilty\n",
    "        policy[key] = p #each state is assigned actions with equal probability\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Creating a dictionary for Q Table (state action value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "        Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Episodic Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "            \n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(0.1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        \n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break   \n",
    "        \n",
    "        state, reward, finished, info =  env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "        \n",
    "        episode.append(timestep)\n",
    "        \n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(0.05)\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Testing policy and displaying win percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 First visit Monte Carlo prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Executing and testing the built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =gym.make('FrozenLakeNotSlippery-v0')\n",
    "#policy = monte_carlo_e_soft(env, episodes=1)\n",
    "test_policy(policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Model with 50 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Model with 100 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 What are your findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "import itertools\n",
    "#import tqdm\n",
    "\n",
    "#tqdm.monitor_interval = 0\n",
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):#for each state\n",
    "        current_end = 0 #???\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n): #for each action corresponding to each state\n",
    "            p[action] = 1 / env.action_space.n #equal probabilty\n",
    "        policy[key] = p #each state is assigned actions with equal probability\n",
    "    return policy\n",
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys(): #for each state\n",
    "         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}#??\n",
    "    return Q\n",
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "            \n",
    "            timestep = []\n",
    "            timestep.append(s)\n",
    "            n = random.uniform(0, sum(policy[s].values()))\n",
    "            top_range = 0\n",
    "            for prob in policy[s].items():\n",
    "                top_range += prob[1]\n",
    "                if n < top_range:\n",
    "                    action = prob[0]\n",
    "                    break \n",
    "            state, reward, finished, info = env.step(action)\n",
    "            timestep.append(action)\n",
    "            timestep.append(reward)\n",
    "            episode.append(timestep)\n",
    "    \n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "        return episode\n",
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r\n",
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {} # 3.\n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        print('hi1')\n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            print('hi2')\n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy\n",
    "env =gym.make('FrozenLake-v0')\n",
    "policy = monte_carlo_e_soft(env, episodes=1)\n",
    "test_policy(policy,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from gym.spaces.tuple_space import Tuple\n",
    "from gym.envs.registration import register\n",
    "import random\n",
    "import itertools\n",
    "#import tqdm\n",
    "\n",
    "#tqdm.monitor_interval = 0\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=200\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200\n",
    ")\n",
    "fl_slippery = {\n",
    "    'small': 'FrozenLake-v0',\n",
    "    'big': 'FrozenLake8x8-v0'\n",
    "}\n",
    "\n",
    "fl_not_slippery = {\n",
    "    'small': 'FrozenLakeNotSlippery-v0',\n",
    "    'big': 'FrozenLakeNotSlippery8x8-v0'\n",
    "}\n",
    "def create_environment(slippery=False, big=False):\n",
    "    if slippery:\n",
    "        env = gym.make(fl_slippery['big'] if big else fl_slippery['small'])\n",
    "    else:\n",
    "        env = gym.make(fl_not_slippery['big'] if big else fl_not_slippery['small'])\n",
    "    env.reset()\n",
    "    return env\n",
    "\n",
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy\n",
    "\n",
    "\n",
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "        Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q    \n",
    "\n",
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "            \n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(0.1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        \n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break   \n",
    "        \n",
    "        state, reward, finished, info =  env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "        \n",
    "        episode.append(timestep)\n",
    "        \n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(0.05)\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r\n",
    "env = create_environment(slippery=True, big=False)\n",
    "_ = run_game(env, create_random_policy(env))\n",
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env) # 1. \n",
    "        \n",
    "    Q = create_state_action_dictionary(env, policy) # 2.\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in range(episodes): # 4.\n",
    "        G = 0 # 5.\n",
    "        episode = run_game(env=env, policy=policy, display=False) # 6.\n",
    "        for i in reversed(range(0, len(episode))): # 7.\n",
    "            s_t, a_t, r_t = episode[i] # 8. \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # 9.\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # 10.\n",
    "                if returns.get(state_action): # 11.\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # 12.\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # 13.\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # 15.\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy\n",
    "env = create_environment(slippery=False, big=False)\n",
    "policy = monte_carlo_e_soft(env, episodes=200)\n",
    "test_policy(policy, env)\n",
    "_ = run_game(env, policy)\n",
    "env = create_environment(slippery=False, big=True)\n",
    "policy = monte_carlo_e_soft(env, episodes=10000)\n",
    "test_policy(policy, env)\n",
    "_ = run_game(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to navigate a car without Reinforcement Learning\n",
    "\n",
    "- OpenAI Gym Library https://gym.openai.com\n",
    "- Contains various models for researchers to practice Reinforcement Learning problems\n",
    "- keras-rl framework\n",
    "\n",
    "![](https://camo.githubusercontent.com/780b18443ca6ff68004fc01b29e59367b5a70300/68747470733a2f2f7170682e66732e71756f726163646e2e6e65742f6d61696e2d71696d672d65666133343639353532386435326463643036633535643564396234366265662d63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "\n",
    "# Creating thr env\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.s = 328\n",
    "\n",
    "\n",
    "# Setting the number of iterations, penalties and reward to zero,\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = []\n",
    "\n",
    "#done represents when our goal is reached\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample() #randomly select a sample\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    # Put each rendered frame into the dictionary for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "# Printing all the possible actions, states, rewards.\n",
    "def frames1(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        from IPython.display import clear_output\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "frames1(frames)\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Dive into Deep Reinforcement Learning - CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a conda environment: /anaconda3/envs/deeplearning\n",
      "Collecting keras-rl\n",
      "Requirement already satisfied: keras>=2.0.7 in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras-rl) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (5.1.2)\n",
      "Requirement already satisfied: h5py in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Installing collected packages: keras-rl\n",
      "Successfully installed keras-rl-0.4.2\n"
     ]
    }
   ],
   "source": [
    "! source activate /anaconda3/envs/deeplearning/\n",
    "! pip install keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anubhavpatrick/miniconda3/envs/style-transfer/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79/5000: episode: 1, duration: 2.784s, episode steps: 79, steps per second: 28, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.428264, mean_absolute_error: 0.495469, mean_q: 0.052853\n",
      "  113/5000: episode: 2, duration: 0.189s, episode steps: 34, steps per second: 180, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.354168, mean_absolute_error: 0.447340, mean_q: 0.189358\n",
      "  163/5000: episode: 3, duration: 0.392s, episode steps: 50, steps per second: 128, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.317497, mean_absolute_error: 0.469040, mean_q: 0.315477\n",
      "  197/5000: episode: 4, duration: 0.303s, episode steps: 34, steps per second: 112, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.270319, mean_absolute_error: 0.495340, mean_q: 0.471355\n",
      "  261/5000: episode: 5, duration: 0.499s, episode steps: 64, steps per second: 128, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.394, 0.861], loss: 0.228531, mean_absolute_error: 0.557957, mean_q: 0.680882\n",
      "  295/5000: episode: 6, duration: 0.192s, episode steps: 34, steps per second: 177, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.207, 0.836], loss: 0.172735, mean_absolute_error: 0.630425, mean_q: 0.942412\n",
      "  326/5000: episode: 7, duration: 0.179s, episode steps: 31, steps per second: 173, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.081 [-0.349, 0.812], loss: 0.142767, mean_absolute_error: 0.706416, mean_q: 1.141936\n",
      "  355/5000: episode: 8, duration: 0.162s, episode steps: 29, steps per second: 179, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.086 [-0.395, 0.805], loss: 0.119537, mean_absolute_error: 0.782606, mean_q: 1.349453\n",
      "  377/5000: episode: 9, duration: 0.146s, episode steps: 22, steps per second: 151, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.353, 0.912], loss: 0.096975, mean_absolute_error: 0.857594, mean_q: 1.546370\n",
      "  393/5000: episode: 10, duration: 0.098s, episode steps: 16, steps per second: 163, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.016], loss: 0.082044, mean_absolute_error: 0.906574, mean_q: 1.707448\n",
      "  414/5000: episode: 11, duration: 0.125s, episode steps: 21, steps per second: 168, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.080 [-0.629, 1.258], loss: 0.092589, mean_absolute_error: 0.991321, mean_q: 1.859154\n",
      "  434/5000: episode: 12, duration: 0.164s, episode steps: 20, steps per second: 122, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.088 [-0.551, 1.064], loss: 0.085752, mean_absolute_error: 1.064436, mean_q: 2.022919\n",
      "  457/5000: episode: 13, duration: 0.196s, episode steps: 23, steps per second: 117, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.083 [-0.543, 0.988], loss: 0.084112, mean_absolute_error: 1.146243, mean_q: 2.209906\n",
      "  475/5000: episode: 14, duration: 0.129s, episode steps: 18, steps per second: 140, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.094 [-0.584, 1.242], loss: 0.092524, mean_absolute_error: 1.244059, mean_q: 2.384669\n",
      "  489/5000: episode: 15, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.606, 1.219], loss: 0.101177, mean_absolute_error: 1.320370, mean_q: 2.543747\n",
      "  501/5000: episode: 16, duration: 0.097s, episode steps: 12, steps per second: 124, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.779, 1.336], loss: 0.130994, mean_absolute_error: 1.390938, mean_q: 2.677539\n",
      "  516/5000: episode: 17, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.078 [-0.960, 1.465], loss: 0.126419, mean_absolute_error: 1.459340, mean_q: 2.811546\n",
      "  531/5000: episode: 18, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.089 [-0.800, 1.397], loss: 0.168509, mean_absolute_error: 1.530947, mean_q: 2.931288\n",
      "  546/5000: episode: 19, duration: 0.085s, episode steps: 15, steps per second: 176, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.116 [-0.767, 1.446], loss: 0.169809, mean_absolute_error: 1.576100, mean_q: 3.061966\n",
      "  558/5000: episode: 20, duration: 0.068s, episode steps: 12, steps per second: 177, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.126 [-1.178, 2.071], loss: 0.151600, mean_absolute_error: 1.632651, mean_q: 3.187694\n",
      "  567/5000: episode: 21, duration: 0.092s, episode steps: 9, steps per second: 98, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-0.964, 1.743], loss: 0.159610, mean_absolute_error: 1.708738, mean_q: 3.333429\n",
      "  577/5000: episode: 22, duration: 0.087s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.133 [-1.191, 1.998], loss: 0.159088, mean_absolute_error: 1.701216, mean_q: 3.372838\n",
      "  586/5000: episode: 23, duration: 0.077s, episode steps: 9, steps per second: 116, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-1.141, 1.974], loss: 0.206889, mean_absolute_error: 1.785701, mean_q: 3.508223\n",
      "  595/5000: episode: 24, duration: 0.080s, episode steps: 9, steps per second: 112, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.137 [-1.161, 1.872], loss: 0.237772, mean_absolute_error: 1.821144, mean_q: 3.611001\n",
      "  605/5000: episode: 25, duration: 0.092s, episode steps: 10, steps per second: 108, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.101 [-1.383, 2.110], loss: 0.381710, mean_absolute_error: 1.951322, mean_q: 3.704563\n",
      "  614/5000: episode: 26, duration: 0.089s, episode steps: 9, steps per second: 102, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.134 [-1.162, 1.966], loss: 0.303215, mean_absolute_error: 1.974834, mean_q: 3.768621\n",
      "  624/5000: episode: 27, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.109 [-1.575, 2.425], loss: 0.254736, mean_absolute_error: 1.957375, mean_q: 3.810384\n",
      "  633/5000: episode: 28, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.127 [-1.421, 2.295], loss: 0.394701, mean_absolute_error: 2.058367, mean_q: 3.936335\n",
      "  643/5000: episode: 29, duration: 0.080s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.146 [-1.132, 1.946], loss: 0.457596, mean_absolute_error: 2.128757, mean_q: 4.046721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  652/5000: episode: 30, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.521, 2.458], loss: 0.400770, mean_absolute_error: 2.126016, mean_q: 4.089945\n",
      "  665/5000: episode: 31, duration: 0.098s, episode steps: 13, steps per second: 132, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.095 [-1.595, 2.428], loss: 0.370391, mean_absolute_error: 2.211614, mean_q: 4.272445\n",
      "  675/5000: episode: 32, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.614, 2.611], loss: 0.444119, mean_absolute_error: 2.291719, mean_q: 4.394015\n",
      "  684/5000: episode: 33, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.737, 2.764], loss: 0.572246, mean_absolute_error: 2.315066, mean_q: 4.453006\n",
      "  693/5000: episode: 34, duration: 0.071s, episode steps: 9, steps per second: 127, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.712, 2.836], loss: 0.669020, mean_absolute_error: 2.407707, mean_q: 4.522391\n",
      "  701/5000: episode: 35, duration: 0.079s, episode steps: 8, steps per second: 101, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.565], loss: 0.397653, mean_absolute_error: 2.351272, mean_q: 4.523032\n",
      "  711/5000: episode: 36, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.977, 3.069], loss: 0.344492, mean_absolute_error: 2.380947, mean_q: 4.670717\n",
      "  721/5000: episode: 37, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.547, 2.430], loss: 0.559390, mean_absolute_error: 2.482370, mean_q: 4.830613\n",
      "  729/5000: episode: 38, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.547, 2.531], loss: 0.513071, mean_absolute_error: 2.467937, mean_q: 4.870761\n",
      "  740/5000: episode: 39, duration: 0.091s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.119 [-1.764, 2.773], loss: 0.703556, mean_absolute_error: 2.603395, mean_q: 4.969615\n",
      "  749/5000: episode: 40, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.774, 2.819], loss: 0.781195, mean_absolute_error: 2.632924, mean_q: 4.987238\n",
      "  762/5000: episode: 41, duration: 0.097s, episode steps: 13, steps per second: 133, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.134 [-1.565, 2.589], loss: 0.545675, mean_absolute_error: 2.609020, mean_q: 4.993820\n",
      "  771/5000: episode: 42, duration: 0.073s, episode steps: 9, steps per second: 124, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.613, 2.468], loss: 0.618452, mean_absolute_error: 2.666546, mean_q: 5.124832\n",
      "  779/5000: episode: 43, duration: 0.060s, episode steps: 8, steps per second: 133, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.168 [-1.333, 2.247], loss: 0.592635, mean_absolute_error: 2.740724, mean_q: 5.225170\n",
      "  791/5000: episode: 44, duration: 0.102s, episode steps: 12, steps per second: 117, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.138, 1.938], loss: 0.736674, mean_absolute_error: 2.791812, mean_q: 5.242654\n",
      "  800/5000: episode: 45, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.145 [-1.136, 1.917], loss: 0.460422, mean_absolute_error: 2.757522, mean_q: 5.293390\n",
      "  809/5000: episode: 46, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.165 [-1.142, 1.971], loss: 0.604678, mean_absolute_error: 2.851313, mean_q: 5.456502\n",
      "  819/5000: episode: 47, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.153 [-1.139, 1.980], loss: 0.626431, mean_absolute_error: 2.855365, mean_q: 5.498691\n",
      "  828/5000: episode: 48, duration: 0.077s, episode steps: 9, steps per second: 117, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.114 [-1.404, 2.216], loss: 0.715274, mean_absolute_error: 2.938082, mean_q: 5.607287\n",
      "  838/5000: episode: 49, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.137 [-1.379, 2.215], loss: 0.823309, mean_absolute_error: 2.991678, mean_q: 5.574740\n",
      "  848/5000: episode: 50, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.099 [-1.390, 2.111], loss: 0.658558, mean_absolute_error: 3.007398, mean_q: 5.620977\n",
      "  859/5000: episode: 51, duration: 0.071s, episode steps: 11, steps per second: 156, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.111 [-1.142, 1.898], loss: 0.876354, mean_absolute_error: 3.074866, mean_q: 5.711570\n",
      "  871/5000: episode: 52, duration: 0.080s, episode steps: 12, steps per second: 151, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.110 [-1.600, 2.501], loss: 0.788741, mean_absolute_error: 3.056662, mean_q: 5.720654\n",
      "  883/5000: episode: 53, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.111 [-1.219, 1.946], loss: 0.765584, mean_absolute_error: 3.095314, mean_q: 5.802703\n",
      "  894/5000: episode: 54, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.105 [-1.181, 1.891], loss: 0.837482, mean_absolute_error: 3.146539, mean_q: 5.859112\n",
      "  905/5000: episode: 55, duration: 0.072s, episode steps: 11, steps per second: 154, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.100 [-1.156, 1.891], loss: 0.839266, mean_absolute_error: 3.175355, mean_q: 5.978429\n",
      "  916/5000: episode: 56, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.100 [-1.205, 1.879], loss: 0.845690, mean_absolute_error: 3.240028, mean_q: 6.070868\n",
      "  926/5000: episode: 57, duration: 0.066s, episode steps: 10, steps per second: 153, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.117 [-0.996, 1.639], loss: 0.757403, mean_absolute_error: 3.229367, mean_q: 6.098854\n",
      "  936/5000: episode: 58, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.148, 1.951], loss: 0.856792, mean_absolute_error: 3.295958, mean_q: 6.171490\n",
      "  945/5000: episode: 59, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.137 [-1.216, 1.992], loss: 0.839978, mean_absolute_error: 3.326308, mean_q: 6.265359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  956/5000: episode: 60, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.217, 1.886], loss: 0.927062, mean_absolute_error: 3.357367, mean_q: 6.305768\n",
      "  966/5000: episode: 61, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.106 [-1.184, 1.774], loss: 0.976928, mean_absolute_error: 3.417622, mean_q: 6.229517\n",
      "  978/5000: episode: 62, duration: 0.079s, episode steps: 12, steps per second: 153, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.174, 1.991], loss: 0.836661, mean_absolute_error: 3.398481, mean_q: 6.306617\n",
      "  989/5000: episode: 63, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.117 [-1.328, 2.113], loss: 0.662922, mean_absolute_error: 3.412726, mean_q: 6.491066\n",
      " 1000/5000: episode: 64, duration: 0.095s, episode steps: 11, steps per second: 116, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.121 [-1.134, 1.888], loss: 1.136665, mean_absolute_error: 3.508247, mean_q: 6.479087\n",
      " 1011/5000: episode: 65, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.100 [-1.380, 2.034], loss: 0.800953, mean_absolute_error: 3.479155, mean_q: 6.491406\n",
      " 1021/5000: episode: 66, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.123 [-1.352, 2.134], loss: 0.890087, mean_absolute_error: 3.526318, mean_q: 6.580848\n",
      " 1033/5000: episode: 67, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.107 [-1.182, 1.949], loss: 0.975844, mean_absolute_error: 3.554528, mean_q: 6.539538\n",
      " 1042/5000: episode: 68, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.131 [-1.155, 1.914], loss: 1.282117, mean_absolute_error: 3.652604, mean_q: 6.608572\n",
      " 1052/5000: episode: 69, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.118 [-1.356, 2.139], loss: 0.480235, mean_absolute_error: 3.517867, mean_q: 6.690019\n",
      " 1063/5000: episode: 70, duration: 0.098s, episode steps: 11, steps per second: 113, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.136 [-1.161, 1.913], loss: 0.746325, mean_absolute_error: 3.560525, mean_q: 6.755852\n",
      " 1072/5000: episode: 71, duration: 0.087s, episode steps: 9, steps per second: 104, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.134 [-1.222, 1.942], loss: 1.145707, mean_absolute_error: 3.702461, mean_q: 6.851940\n",
      " 1081/5000: episode: 72, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.143 [-0.955, 1.756], loss: 1.068959, mean_absolute_error: 3.715625, mean_q: 6.855356\n",
      " 1092/5000: episode: 73, duration: 0.061s, episode steps: 11, steps per second: 181, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.099 [-1.221, 1.933], loss: 1.108078, mean_absolute_error: 3.738664, mean_q: 6.767151\n",
      " 1101/5000: episode: 74, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.165 [-1.339, 2.320], loss: 0.931907, mean_absolute_error: 3.708216, mean_q: 6.784574\n",
      " 1110/5000: episode: 75, duration: 0.059s, episode steps: 9, steps per second: 151, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.544, 2.451], loss: 0.829486, mean_absolute_error: 3.691813, mean_q: 6.808308\n",
      " 1120/5000: episode: 76, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.156 [-1.136, 1.992], loss: 0.682835, mean_absolute_error: 3.712216, mean_q: 6.947370\n",
      " 1133/5000: episode: 77, duration: 0.116s, episode steps: 13, steps per second: 112, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.098 [-1.343, 2.067], loss: 0.848865, mean_absolute_error: 3.736594, mean_q: 6.993009\n",
      " 1142/5000: episode: 78, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.152 [-1.135, 1.939], loss: 0.842401, mean_absolute_error: 3.778078, mean_q: 6.988471\n",
      " 1151/5000: episode: 79, duration: 0.073s, episode steps: 9, steps per second: 123, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.530, 2.455], loss: 1.064345, mean_absolute_error: 3.790962, mean_q: 6.872744\n",
      " 1161/5000: episode: 80, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.524, 2.527], loss: 1.237637, mean_absolute_error: 3.863445, mean_q: 6.939331\n",
      " 1170/5000: episode: 81, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.748, 2.806], loss: 0.751246, mean_absolute_error: 3.782057, mean_q: 6.985559\n",
      " 1180/5000: episode: 82, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.912, 3.055], loss: 0.703407, mean_absolute_error: 3.796491, mean_q: 7.138577\n",
      " 1190/5000: episode: 83, duration: 0.082s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.121 [-1.393, 2.169], loss: 0.801073, mean_absolute_error: 3.828299, mean_q: 7.131027\n",
      " 1201/5000: episode: 84, duration: 0.137s, episode steps: 11, steps per second: 80, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.128 [-1.134, 1.931], loss: 0.859239, mean_absolute_error: 3.871146, mean_q: 7.177628\n",
      " 1222/5000: episode: 85, duration: 0.156s, episode steps: 21, steps per second: 135, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.098 [-0.600, 0.977], loss: 0.734524, mean_absolute_error: 3.906941, mean_q: 7.235184\n",
      " 1253/5000: episode: 86, duration: 0.238s, episode steps: 31, steps per second: 130, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.092 [-0.381, 0.808], loss: 0.936414, mean_absolute_error: 3.943421, mean_q: 7.199473\n",
      " 1326/5000: episode: 87, duration: 0.495s, episode steps: 73, steps per second: 147, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.090 [-0.561, 0.709], loss: 0.847641, mean_absolute_error: 4.042167, mean_q: 7.476311\n",
      " 1356/5000: episode: 88, duration: 0.215s, episode steps: 30, steps per second: 140, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.113 [-0.812, 0.170], loss: 0.899399, mean_absolute_error: 4.169217, mean_q: 7.738962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1384/5000: episode: 89, duration: 0.201s, episode steps: 28, steps per second: 139, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-0.932, 0.366], loss: 0.858233, mean_absolute_error: 4.234887, mean_q: 7.852984\n",
      " 1411/5000: episode: 90, duration: 0.214s, episode steps: 27, steps per second: 126, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.088 [-1.228, 0.457], loss: 0.915996, mean_absolute_error: 4.282281, mean_q: 7.987362\n",
      " 1428/5000: episode: 91, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.101 [-2.001, 1.045], loss: 0.797542, mean_absolute_error: 4.333748, mean_q: 8.112404\n",
      " 1438/5000: episode: 92, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.580, 1.546], loss: 0.835683, mean_absolute_error: 4.363051, mean_q: 8.157490\n",
      " 1447/5000: episode: 93, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.139 [-1.863, 1.172], loss: 1.490089, mean_absolute_error: 4.468218, mean_q: 8.245633\n",
      " 1458/5000: episode: 94, duration: 0.065s, episode steps: 11, steps per second: 168, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.120 [-2.278, 1.352], loss: 0.851289, mean_absolute_error: 4.429702, mean_q: 8.266669\n",
      " 1538/5000: episode: 95, duration: 0.464s, episode steps: 80, steps per second: 172, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.111 [-1.314, 0.825], loss: 1.121066, mean_absolute_error: 4.553621, mean_q: 8.525770\n",
      " 1572/5000: episode: 96, duration: 0.186s, episode steps: 34, steps per second: 183, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-0.949, 0.270], loss: 1.064757, mean_absolute_error: 4.733032, mean_q: 8.851727\n",
      " 1595/5000: episode: 97, duration: 0.184s, episode steps: 23, steps per second: 125, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.108 [-1.029, 0.379], loss: 1.064225, mean_absolute_error: 4.850844, mean_q: 9.167097\n",
      " 1681/5000: episode: 98, duration: 0.634s, episode steps: 86, steps per second: 136, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.176 [-0.516, 0.932], loss: 1.132668, mean_absolute_error: 4.926318, mean_q: 9.291959\n",
      " 1709/5000: episode: 99, duration: 0.186s, episode steps: 28, steps per second: 151, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.060 [-1.311, 0.595], loss: 1.062472, mean_absolute_error: 5.046735, mean_q: 9.544882\n",
      " 1726/5000: episode: 100, duration: 0.117s, episode steps: 17, steps per second: 145, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.121 [-1.381, 0.564], loss: 1.352215, mean_absolute_error: 5.236290, mean_q: 9.955822\n",
      " 1738/5000: episode: 101, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.119 [-1.276, 0.791], loss: 0.749950, mean_absolute_error: 5.156779, mean_q: 9.932344\n",
      " 1752/5000: episode: 102, duration: 0.077s, episode steps: 14, steps per second: 183, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.244, 0.589], loss: 1.751069, mean_absolute_error: 5.287486, mean_q: 10.028138\n",
      " 1764/5000: episode: 103, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.096 [-1.202, 0.607], loss: 1.427467, mean_absolute_error: 5.337630, mean_q: 10.050311\n",
      " 1784/5000: episode: 104, duration: 0.112s, episode steps: 20, steps per second: 178, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.069 [-1.375, 0.791], loss: 1.071077, mean_absolute_error: 5.349923, mean_q: 10.151894\n",
      " 1797/5000: episode: 105, duration: 0.073s, episode steps: 13, steps per second: 179, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.118 [-1.311, 0.586], loss: 1.178182, mean_absolute_error: 5.346656, mean_q: 10.154252\n",
      " 1813/5000: episode: 106, duration: 0.128s, episode steps: 16, steps per second: 125, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.105 [-1.096, 0.584], loss: 1.552672, mean_absolute_error: 5.459437, mean_q: 10.326311\n",
      " 1829/5000: episode: 107, duration: 0.120s, episode steps: 16, steps per second: 133, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.078 [-1.232, 0.616], loss: 1.729021, mean_absolute_error: 5.486218, mean_q: 10.348064\n",
      " 1844/5000: episode: 108, duration: 0.138s, episode steps: 15, steps per second: 109, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.071 [-1.257, 0.645], loss: 1.919225, mean_absolute_error: 5.515069, mean_q: 10.357631\n",
      " 1864/5000: episode: 109, duration: 0.158s, episode steps: 20, steps per second: 127, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.060 [-1.157, 0.633], loss: 2.177475, mean_absolute_error: 5.656678, mean_q: 10.568531\n",
      " 1880/5000: episode: 110, duration: 0.129s, episode steps: 16, steps per second: 124, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.103 [-1.228, 0.602], loss: 0.994461, mean_absolute_error: 5.565639, mean_q: 10.620613\n",
      " 1894/5000: episode: 111, duration: 0.102s, episode steps: 14, steps per second: 137, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.111 [-1.509, 0.783], loss: 1.917957, mean_absolute_error: 5.721172, mean_q: 10.792364\n",
      " 1907/5000: episode: 112, duration: 0.096s, episode steps: 13, steps per second: 135, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.102 [-1.802, 1.008], loss: 1.320722, mean_absolute_error: 5.708048, mean_q: 10.889677\n",
      " 1920/5000: episode: 113, duration: 0.101s, episode steps: 13, steps per second: 128, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.092 [-2.307, 1.401], loss: 2.317964, mean_absolute_error: 5.807928, mean_q: 10.941279\n",
      " 1934/5000: episode: 114, duration: 0.105s, episode steps: 14, steps per second: 134, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.120 [-2.002, 1.146], loss: 1.190240, mean_absolute_error: 5.712420, mean_q: 10.917348\n",
      " 1948/5000: episode: 115, duration: 0.109s, episode steps: 14, steps per second: 128, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.108 [-1.451, 0.757], loss: 1.391198, mean_absolute_error: 5.761964, mean_q: 11.006845\n",
      " 1965/5000: episode: 116, duration: 0.126s, episode steps: 17, steps per second: 135, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.077 [-1.221, 0.751], loss: 1.449565, mean_absolute_error: 5.871063, mean_q: 11.264035\n",
      " 1977/5000: episode: 117, duration: 0.094s, episode steps: 12, steps per second: 127, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.120 [-1.266, 0.609], loss: 2.246802, mean_absolute_error: 6.017084, mean_q: 11.405040\n",
      " 1988/5000: episode: 118, duration: 0.091s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.125 [-1.442, 0.766], loss: 3.457945, mean_absolute_error: 6.108458, mean_q: 11.250617\n",
      " 1998/5000: episode: 119, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.132 [-1.516, 0.804], loss: 1.283066, mean_absolute_error: 5.795885, mean_q: 10.957404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2008/5000: episode: 120, duration: 0.088s, episode steps: 10, steps per second: 114, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.125 [-1.521, 0.838], loss: 2.409948, mean_absolute_error: 5.881581, mean_q: 10.940340\n",
      " 2026/5000: episode: 121, duration: 0.153s, episode steps: 18, steps per second: 118, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.096 [-1.407, 0.753], loss: 1.813440, mean_absolute_error: 5.991102, mean_q: 11.368196\n",
      " 2051/5000: episode: 122, duration: 0.192s, episode steps: 25, steps per second: 130, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.060 [-1.059, 0.571], loss: 1.901778, mean_absolute_error: 6.235159, mean_q: 11.848960\n",
      " 2066/5000: episode: 123, duration: 0.115s, episode steps: 15, steps per second: 131, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.119 [-1.000, 0.369], loss: 2.026040, mean_absolute_error: 6.131616, mean_q: 11.609079\n",
      " 2094/5000: episode: 124, duration: 0.193s, episode steps: 28, steps per second: 145, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.045 [-1.182, 0.601], loss: 2.369617, mean_absolute_error: 6.248060, mean_q: 11.758738\n",
      " 2109/5000: episode: 125, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.092 [-1.254, 0.597], loss: 1.650917, mean_absolute_error: 6.245438, mean_q: 11.863241\n",
      " 2121/5000: episode: 126, duration: 0.091s, episode steps: 12, steps per second: 131, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.115 [-1.234, 0.627], loss: 1.804600, mean_absolute_error: 6.354250, mean_q: 12.045578\n",
      " 2138/5000: episode: 127, duration: 0.128s, episode steps: 17, steps per second: 133, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.099 [-1.135, 0.447], loss: 2.321272, mean_absolute_error: 6.372330, mean_q: 12.038302\n",
      " 2164/5000: episode: 128, duration: 0.184s, episode steps: 26, steps per second: 141, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.047 [-1.063, 0.458], loss: 2.140193, mean_absolute_error: 6.325882, mean_q: 11.941402\n",
      " 2189/5000: episode: 129, duration: 0.184s, episode steps: 25, steps per second: 136, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.070 [-1.356, 0.588], loss: 2.358887, mean_absolute_error: 6.440199, mean_q: 12.131061\n",
      " 2214/5000: episode: 130, duration: 0.195s, episode steps: 25, steps per second: 128, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.059 [-0.925, 0.384], loss: 2.772784, mean_absolute_error: 6.534181, mean_q: 12.145221\n",
      " 2239/5000: episode: 131, duration: 0.185s, episode steps: 25, steps per second: 135, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.097 [-0.957, 0.378], loss: 2.158588, mean_absolute_error: 6.483061, mean_q: 12.171274\n",
      " 2262/5000: episode: 132, duration: 0.162s, episode steps: 23, steps per second: 142, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.051 [-1.013, 0.424], loss: 2.089000, mean_absolute_error: 6.467677, mean_q: 12.262227\n",
      " 2286/5000: episode: 133, duration: 0.185s, episode steps: 24, steps per second: 130, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.064 [-1.141, 0.445], loss: 2.303560, mean_absolute_error: 6.645374, mean_q: 12.551697\n",
      " 2305/5000: episode: 134, duration: 0.110s, episode steps: 19, steps per second: 173, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.091 [-1.065, 0.433], loss: 2.801694, mean_absolute_error: 6.685668, mean_q: 12.550326\n",
      " 2320/5000: episode: 135, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.109 [-1.393, 0.591], loss: 3.217441, mean_absolute_error: 6.752518, mean_q: 12.569127\n",
      " 2338/5000: episode: 136, duration: 0.111s, episode steps: 18, steps per second: 162, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.091 [-1.214, 0.558], loss: 1.716816, mean_absolute_error: 6.581356, mean_q: 12.534049\n",
      " 2355/5000: episode: 137, duration: 0.133s, episode steps: 17, steps per second: 128, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.097 [-1.039, 0.412], loss: 1.736068, mean_absolute_error: 6.668468, mean_q: 12.790430\n",
      " 2376/5000: episode: 138, duration: 0.158s, episode steps: 21, steps per second: 133, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.087 [-0.923, 0.421], loss: 3.062828, mean_absolute_error: 6.785982, mean_q: 12.786985\n",
      " 2402/5000: episode: 139, duration: 0.189s, episode steps: 26, steps per second: 137, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.916, 0.541], loss: 2.230689, mean_absolute_error: 6.738311, mean_q: 12.715860\n",
      " 2425/5000: episode: 140, duration: 0.188s, episode steps: 23, steps per second: 122, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.100 [-0.967, 0.396], loss: 2.382822, mean_absolute_error: 6.865917, mean_q: 13.058268\n",
      " 2463/5000: episode: 141, duration: 0.259s, episode steps: 38, steps per second: 147, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-0.991, 0.388], loss: 2.322234, mean_absolute_error: 6.888354, mean_q: 13.106168\n",
      " 2484/5000: episode: 142, duration: 0.140s, episode steps: 21, steps per second: 150, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.109 [-0.994, 0.212], loss: 2.567415, mean_absolute_error: 6.935563, mean_q: 13.148470\n",
      " 2538/5000: episode: 143, duration: 0.384s, episode steps: 54, steps per second: 141, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.888, 0.621], loss: 2.551866, mean_absolute_error: 6.980429, mean_q: 13.239518\n",
      " 2571/5000: episode: 144, duration: 0.249s, episode steps: 33, steps per second: 133, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.074 [-0.872, 0.368], loss: 2.294861, mean_absolute_error: 7.103122, mean_q: 13.604753\n",
      " 2613/5000: episode: 145, duration: 0.329s, episode steps: 42, steps per second: 128, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.028 [-0.800, 0.412], loss: 2.525892, mean_absolute_error: 7.218052, mean_q: 13.774419\n",
      " 2638/5000: episode: 146, duration: 0.179s, episode steps: 25, steps per second: 140, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.131 [-0.845, 0.376], loss: 3.038888, mean_absolute_error: 7.286254, mean_q: 13.810383\n",
      " 2667/5000: episode: 147, duration: 0.200s, episode steps: 29, steps per second: 145, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.108 [-0.811, 0.365], loss: 3.023736, mean_absolute_error: 7.361778, mean_q: 13.954942\n",
      " 2718/5000: episode: 148, duration: 0.317s, episode steps: 51, steps per second: 161, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.048 [-0.869, 0.206], loss: 2.241825, mean_absolute_error: 7.370544, mean_q: 14.147282\n",
      " 2741/5000: episode: 149, duration: 0.163s, episode steps: 23, steps per second: 141, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.092 [-0.830, 0.222], loss: 2.851838, mean_absolute_error: 7.451699, mean_q: 14.213080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2770/5000: episode: 150, duration: 0.237s, episode steps: 29, steps per second: 122, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.101 [-0.980, 0.397], loss: 2.655900, mean_absolute_error: 7.446999, mean_q: 14.270243\n",
      " 2808/5000: episode: 151, duration: 0.266s, episode steps: 38, steps per second: 143, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-0.900, 0.210], loss: 2.780445, mean_absolute_error: 7.651926, mean_q: 14.665781\n",
      " 2855/5000: episode: 152, duration: 0.382s, episode steps: 47, steps per second: 123, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.083 [-0.790, 0.260], loss: 2.422928, mean_absolute_error: 7.716018, mean_q: 14.844281\n",
      " 2893/5000: episode: 153, duration: 0.218s, episode steps: 38, steps per second: 174, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.072 [-0.905, 0.399], loss: 2.870811, mean_absolute_error: 7.829970, mean_q: 14.993472\n",
      " 2930/5000: episode: 154, duration: 0.260s, episode steps: 37, steps per second: 142, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.090 [-0.880, 0.197], loss: 2.600151, mean_absolute_error: 7.889947, mean_q: 15.222677\n",
      " 2972/5000: episode: 155, duration: 0.307s, episode steps: 42, steps per second: 137, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.065 [-1.019, 0.239], loss: 2.600704, mean_absolute_error: 7.901359, mean_q: 15.167885\n",
      " 2998/5000: episode: 156, duration: 0.174s, episode steps: 26, steps per second: 150, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.935, 0.387], loss: 3.851580, mean_absolute_error: 8.005312, mean_q: 15.174066\n",
      " 3043/5000: episode: 157, duration: 0.313s, episode steps: 45, steps per second: 144, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.118 [-0.395, 0.764], loss: 2.796421, mean_absolute_error: 8.077245, mean_q: 15.443452\n",
      " 3074/5000: episode: 158, duration: 0.244s, episode steps: 31, steps per second: 127, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.088 [-0.788, 0.216], loss: 3.099136, mean_absolute_error: 8.116143, mean_q: 15.534355\n",
      " 3173/5000: episode: 159, duration: 0.648s, episode steps: 99, steps per second: 153, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.039 [-0.740, 0.608], loss: 3.417872, mean_absolute_error: 8.268465, mean_q: 15.721898\n",
      " 3194/5000: episode: 160, duration: 0.119s, episode steps: 21, steps per second: 177, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.091 [-0.919, 0.638], loss: 3.260654, mean_absolute_error: 8.335656, mean_q: 15.856836\n",
      " 3261/5000: episode: 161, duration: 0.425s, episode steps: 67, steps per second: 158, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.150 [-0.435, 0.813], loss: 2.589458, mean_absolute_error: 8.416405, mean_q: 16.193926\n",
      " 3317/5000: episode: 162, duration: 0.390s, episode steps: 56, steps per second: 144, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.154 [-0.360, 0.839], loss: 3.015947, mean_absolute_error: 8.471762, mean_q: 16.275314\n",
      " 3345/5000: episode: 163, duration: 0.192s, episode steps: 28, steps per second: 146, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.796, 0.351], loss: 2.980692, mean_absolute_error: 8.673430, mean_q: 16.678711\n",
      " 3440/5000: episode: 164, duration: 0.588s, episode steps: 95, steps per second: 162, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.146 [-0.345, 1.373], loss: 3.209246, mean_absolute_error: 8.719785, mean_q: 16.755119\n",
      " 3479/5000: episode: 165, duration: 0.280s, episode steps: 39, steps per second: 139, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.099 [-0.987, 0.201], loss: 3.406325, mean_absolute_error: 8.811705, mean_q: 16.868073\n",
      " 3575/5000: episode: 166, duration: 0.619s, episode steps: 96, steps per second: 155, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.021 [-0.740, 0.341], loss: 3.477690, mean_absolute_error: 8.898705, mean_q: 17.085962\n",
      " 3631/5000: episode: 167, duration: 0.381s, episode steps: 56, steps per second: 147, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.054 [-0.712, 0.296], loss: 3.063799, mean_absolute_error: 9.009459, mean_q: 17.395794\n",
      " 3675/5000: episode: 168, duration: 0.276s, episode steps: 44, steps per second: 159, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.086 [-0.807, 0.433], loss: 3.108279, mean_absolute_error: 9.225960, mean_q: 17.836040\n",
      " 3714/5000: episode: 169, duration: 0.277s, episode steps: 39, steps per second: 141, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.126 [-0.856, 0.338], loss: 3.836146, mean_absolute_error: 9.146949, mean_q: 17.547894\n",
      " 3759/5000: episode: 170, duration: 0.328s, episode steps: 45, steps per second: 137, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.084 [-0.811, 0.291], loss: 2.631188, mean_absolute_error: 9.246676, mean_q: 17.908218\n",
      " 3795/5000: episode: 171, duration: 0.261s, episode steps: 36, steps per second: 138, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.093 [-0.754, 0.360], loss: 3.460049, mean_absolute_error: 9.395716, mean_q: 18.145733\n",
      " 3841/5000: episode: 172, duration: 0.333s, episode steps: 46, steps per second: 138, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.056 [-0.700, 0.242], loss: 4.003126, mean_absolute_error: 9.413713, mean_q: 18.084675\n",
      " 3873/5000: episode: 173, duration: 0.210s, episode steps: 32, steps per second: 153, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.111 [-0.864, 0.437], loss: 2.922576, mean_absolute_error: 9.436025, mean_q: 18.301479\n",
      " 3935/5000: episode: 174, duration: 0.378s, episode steps: 62, steps per second: 164, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.112 [-0.304, 0.711], loss: 3.728346, mean_absolute_error: 9.517977, mean_q: 18.362686\n",
      " 4057/5000: episode: 175, duration: 0.932s, episode steps: 122, steps per second: 131, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.029 [-0.806, 0.596], loss: 3.690971, mean_absolute_error: 9.672344, mean_q: 18.639313\n",
      " 4083/5000: episode: 176, duration: 0.168s, episode steps: 26, steps per second: 155, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.126 [-0.690, 0.370], loss: 3.309994, mean_absolute_error: 9.786735, mean_q: 18.946350\n",
      " 4114/5000: episode: 177, duration: 0.206s, episode steps: 31, steps per second: 151, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.082 [-0.804, 0.235], loss: 3.096750, mean_absolute_error: 9.836840, mean_q: 19.042427\n",
      " 4190/5000: episode: 178, duration: 0.504s, episode steps: 76, steps per second: 151, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.037 [-0.767, 0.565], loss: 4.073102, mean_absolute_error: 9.898354, mean_q: 19.083138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4246/5000: episode: 179, duration: 0.387s, episode steps: 56, steps per second: 145, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.030 [-0.914, 0.398], loss: 3.463470, mean_absolute_error: 9.976578, mean_q: 19.302504\n",
      " 4271/5000: episode: 180, duration: 0.145s, episode steps: 25, steps per second: 172, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.112 [-0.697, 0.222], loss: 4.017541, mean_absolute_error: 10.098116, mean_q: 19.530869\n",
      " 4311/5000: episode: 181, duration: 0.271s, episode steps: 40, steps per second: 147, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.331, 0.450], loss: 4.354545, mean_absolute_error: 10.136434, mean_q: 19.468922\n",
      " 4354/5000: episode: 182, duration: 0.320s, episode steps: 43, steps per second: 134, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.119 [-0.874, 0.244], loss: 4.375960, mean_absolute_error: 10.068446, mean_q: 19.325647\n",
      " 4382/5000: episode: 183, duration: 0.181s, episode steps: 28, steps per second: 154, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.108 [-0.698, 0.196], loss: 4.002275, mean_absolute_error: 10.132710, mean_q: 19.567083\n",
      " 4410/5000: episode: 184, duration: 0.199s, episode steps: 28, steps per second: 141, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.129 [-0.732, 0.157], loss: 3.384700, mean_absolute_error: 10.295313, mean_q: 19.934671\n",
      " 4463/5000: episode: 185, duration: 0.377s, episode steps: 53, steps per second: 141, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.075 [-0.642, 0.406], loss: 3.869339, mean_absolute_error: 10.320779, mean_q: 19.990557\n",
      " 4516/5000: episode: 186, duration: 0.396s, episode steps: 53, steps per second: 134, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.064 [-0.740, 0.395], loss: 4.780483, mean_absolute_error: 10.323723, mean_q: 19.832853\n",
      " 4546/5000: episode: 187, duration: 0.197s, episode steps: 30, steps per second: 153, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.113 [-0.797, 0.235], loss: 3.891323, mean_absolute_error: 10.411092, mean_q: 20.073643\n",
      " 4592/5000: episode: 188, duration: 0.346s, episode steps: 46, steps per second: 133, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.085 [-0.703, 0.165], loss: 4.322720, mean_absolute_error: 10.539191, mean_q: 20.303698\n",
      " 4711/5000: episode: 189, duration: 0.768s, episode steps: 119, steps per second: 155, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.012 [-0.633, 0.518], loss: 3.864191, mean_absolute_error: 10.530377, mean_q: 20.351713\n",
      " 4754/5000: episode: 190, duration: 0.270s, episode steps: 43, steps per second: 159, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.111 [-0.903, 0.609], loss: 3.530306, mean_absolute_error: 10.618567, mean_q: 20.620293\n",
      " 4825/5000: episode: 191, duration: 0.448s, episode steps: 71, steps per second: 158, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.062 [-0.634, 0.389], loss: 3.689152, mean_absolute_error: 10.629431, mean_q: 20.619934\n",
      " 4868/5000: episode: 192, duration: 0.315s, episode steps: 43, steps per second: 136, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.104 [-0.664, 0.360], loss: 3.870423, mean_absolute_error: 10.688306, mean_q: 20.696058\n",
      " 4902/5000: episode: 193, duration: 0.202s, episode steps: 34, steps per second: 168, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.119 [-0.745, 0.183], loss: 3.691371, mean_absolute_error: 10.746925, mean_q: 20.852346\n",
      " 4941/5000: episode: 194, duration: 0.291s, episode steps: 39, steps per second: 134, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.115 [-0.607, 0.181], loss: 4.719300, mean_absolute_error: 10.923852, mean_q: 20.996517\n",
      " 4986/5000: episode: 195, duration: 0.317s, episode steps: 45, steps per second: 142, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.121 [-0.764, 0.246], loss: 3.554252, mean_absolute_error: 10.818030, mean_q: 20.934608\n",
      "done, took 37.686 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 50.000, steps: 50\n",
      "Episode 2: reward: 61.000, steps: 61\n",
      "Episode 3: reward: 165.000, steps: 165\n",
      "Episode 4: reward: 43.000, steps: 43\n",
      "Episode 5: reward: 126.000, steps: 126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146377c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Src https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
