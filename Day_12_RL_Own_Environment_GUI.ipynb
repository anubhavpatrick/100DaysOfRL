{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#May require to install opencv if it is not available: conda install -c menpo opencv\n",
    "\n",
    "#REMEMBER: Open CV used BGR image encoding format\n",
    "import cv2\n",
    "\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import pickle #to save and load python objects\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style \n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the constants\n",
    "SIZE = 10 #Grid size of environment will be 10*10\n",
    "EPISODES = 25000\n",
    "MOVE_PENALTY = 1\n",
    "ENEMY_PENALTY = 300\n",
    "FOOD_REWARD = 25\n",
    "epsilon = 1 #epsilon greedy policy\n",
    "EPS_DECAY = 0.9998\n",
    "SHOW_EVERY = 3000\n",
    "start_q_table = None #can be prior q table saved using pickle library\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_N=1\n",
    "FOOD_N=2\n",
    "ENEMY_N=3\n",
    "#open cv using BGR format for image representation\n",
    "d={1:(255,0,0), #Player is Blue\n",
    "   2:(0,255,0), #Food is Green\n",
    "   3:(0,0,255)  #Enemy is Red\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating class for player, food and enemy\n",
    "class Blob:\n",
    "    def __init__(self): #Python constructor\n",
    "        self.x = np.random.randint(0,SIZE)\n",
    "        self.y = np.random.randint(0,SIZE)\n",
    "        #To Do: There can be a issue in which enemy and/or player and/or food land on the same cell\n",
    "    \n",
    "    #for debugging purposes\n",
    "    def __str__(self):\n",
    "        return f\"{self.x}, {self.y}\"\n",
    "    \n",
    "    #operator overloading\n",
    "    def __sub__(self,second):\n",
    "        return (self.x - second.x, self.y-second.y)\n",
    "        \n",
    "    #defining possible actions by Blob agent\n",
    "    def action(self,choice): #very simplified discrete action space consisting of four actions only\n",
    "        if choice == 0:\n",
    "            self.move(x=1,y=1)\n",
    "        elif choice==1:\n",
    "            self.move(x=-1,y=-1)\n",
    "        elif choice ==2:\n",
    "            self.move(x=-1,y=1)\n",
    "        elif choice ==3:\n",
    "            self.move(x=1,y=-1)\n",
    "        elif choice == 4:#Move right\n",
    "            self.move(x=1,y=0)\n",
    "        elif choice ==5 :#Move left\n",
    "            self.move(x=-1,y=0)\n",
    "        elif choice ==6: #Move Up\n",
    "            self.move(x=0,y=1)\n",
    "        elif choice ==7: #Move Down\n",
    "            self.move(x=0,y=-1)\n",
    "        #To Do: Add more choices\n",
    "    \n",
    "    def move(self, x=False,y=False):\n",
    "        #The agent will move either randomly or based on value passed in x or y\n",
    "        #if not x:#x is local var and self.x is class var\n",
    "        #    self.x = np.random.randint(-1,2)  \n",
    "        #else:\n",
    "        self.x += x\n",
    "            \n",
    "        #if not y:\n",
    "        #    self.y = np.random.randint(-1,2)  \n",
    "        #else:\n",
    "        self.y += y\n",
    "        \n",
    "        #We have to also ensure that blob does not move outside the boundaries\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > (SIZE-1):\n",
    "            self.x = SIZE-1\n",
    "        \n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > (SIZE-1):\n",
    "            self.y = SIZE-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#States of our Q Table consist of difference between x and y coordinates of the player and food Blob AND player and enemy Blob\n",
    "if start_q_table is None:\n",
    "    q_table = {} #dictionary\n",
    "    for x1 in range(-SIZE+1, SIZE):\n",
    "        for y1 in range(-SIZE+1, SIZE):\n",
    "            for x2 in range(-SIZE+1, SIZE):\n",
    "                for y2 in range(-SIZE+1,SIZE):\n",
    "                    q_table[((x1,y1),(x2,y2))]= [np.random.uniform(-5,0) for i in range(8)] #since there are eight discrete actions\n",
    "                    #The initial values need to be modified to see the impact\n",
    "                    \n",
    "else: #The Q Table exists and may be present in the form of pickle object\n",
    "    with open(start_q_table,\"rb\") as f:\n",
    "        q_table = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on #0, epsilon: -8.000640078544932e-05\n",
      "3000 ep mean nan\n",
      "on #3000, epsilon: -6.080972937527747e-05\n",
      "3000 ep mean -0.18533333333333332\n",
      "on #6000, epsilon: -4.1606656940980924e-05\n",
      "3000 ep mean -0.15933333333333333\n",
      "on #9000, epsilon: -2.2403584506680086e-05\n",
      "3000 ep mean -0.05533333333333333\n",
      "on #12000, epsilon: -3.2005120723765297e-06\n",
      "3000 ep mean -0.09\n",
      "on #15000, epsilon: 6.401024152037934e-09\n",
      "3000 ep mean -0.116\n",
      "on #18000, epsilon: 6.401024152037934e-09\n",
      "3000 ep mean -0.23733333333333334\n",
      "on #21000, epsilon: 6.401024152037934e-09\n",
      "3000 ep mean 0.13533333333333333\n",
      "on #24000, epsilon: 6.401024152037934e-09\n",
      "3000 ep mean 0.222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-64542f95ffd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "START_EPSILON_DECAYING = 1#From which episode we want to start to decay epsilon\n",
    "END_EPSILON_DECAYING = EPISODES // 2 #Till which episode we want to decay epsilon\n",
    "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "episode_rewards = []\n",
    "for episode in range(EPISODES):\n",
    "    player = Blob()\n",
    "    food = Blob()\n",
    "    enemy = Blob()\n",
    "    \n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f'on #{episode}, epsilon: {epsilon}')\n",
    "        print(f'{SHOW_EVERY} ep mean {np.mean(episode_rewards[-SHOW_EVERY: ])}')\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "    \n",
    "    #updating epsilon value\n",
    "    \n",
    "    episode_reward = 0\n",
    "    for i in range(200): #Here 200 is the steps in each episode. It is a hyperparameter\n",
    "        obs = (player - food, player - enemy)#Remember the function overriding\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[obs])\n",
    "        else:\n",
    "            action = np.random.randint(0,8)\n",
    "        \n",
    "        player.action(action)\n",
    "       \n",
    "        #Deciding the reward or penalty of agent after every step in each episode\n",
    "        if player.x == enemy.x and player.y == enemy.y:\n",
    "            reward = -ENEMY_PENALTY\n",
    "        elif player.x == food.x and player.y == food.y:\n",
    "            reward = FOOD_REWARD\n",
    "        else:\n",
    "            reward = -MOVE_PENALTY\n",
    "            \n",
    "        new_obs = (player-food,player-enemy)\n",
    "        max_future_q = np.max(q_table[new_obs])\n",
    "        current_q = q_table[obs][action]\n",
    "    \n",
    "        if reward == FOOD_REWARD:\n",
    "            new_q = FOOD_REWARD\n",
    "            q_table[obs][action] = new_q\n",
    "            episode_rewards.append(reward)\n",
    "            break\n",
    "        elif reward == -ENEMY_PENALTY:\n",
    "            new_q = -ENEMY_PENALTY\n",
    "            q_table[obs][action] = new_q\n",
    "            episode_rewards.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[obs][action] = new_q\n",
    "            episode_rewards.append(reward)\n",
    "        \n",
    "        #Code to display the environment\n",
    "        if episode>24000:\n",
    "            env=np.zeros((SIZE,SIZE,3),dtype=np.uint8)\n",
    "            env[food.y][food.x] = d[FOOD_N]\n",
    "            env[player.y][player.x] = d[PLAYER_N]\n",
    "            env[enemy.y][enemy.x] = d[ENEMY_N]            \n",
    "            \n",
    "            img = Image.fromarray(env,'RGB')#from PIL library\n",
    "            img = img.resize((400,400))\n",
    "            cv2.imshow('',np.array(img))\n",
    "            cv2.waitKey(100)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "not x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
